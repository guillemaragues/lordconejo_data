{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Collecting tzdata>=2022.7\n",
            "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\34695\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\34695\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Installing collected packages: urllib3, tzdata, pytz, numpy, idna, charset-normalizer, certifi, requests, pandas\n",
            "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 numpy-2.2.3 pandas-2.2.3 pytz-2025.1 requests-2.32.3 tzdata-2025.1 urllib3-2.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\34695\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\34695\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\34695\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "1SGPUi_PqXNf",
        "outputId": "0aede555-a340-4cc9-f03e-46976738b205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': [{'name': 'impressions', 'period': 'day', 'title': 'Impressions', 'description': 'The number of times that your posts, stories, reels, videos and live videos were on screen, including in ads.', 'total_value': {'value': 1555}, 'id': '17841409297279102/insights/impressions/day'}, {'name': 'reach', 'period': 'day', 'title': 'Accounts reached', 'description': 'The number of unique accounts that have seen your content, at least once, including in ads. Content includes posts, stories, reels, videos and live videos. Reach is different from impressions, which may include multiple views of your content by the same accounts. This metric is estimated and in development.', 'total_value': {'value': 616}, 'id': '17841409297279102/insights/reach/day'}, {'name': 'total_interactions', 'period': 'day', 'title': 'Content interactions', 'description': 'The total number of post interactions, story interactions, reels interactions, video interactions and live video interactions, including any interactions on boosted content.', 'total_value': {'value': 157}, 'id': '17841409297279102/insights/total_interactions/day'}, {'name': 'accounts_engaged', 'period': 'day', 'title': 'Accounts engaged', 'description': 'The number of accounts that have interacted with your content, including in ads. Content includes posts, stories, reels, videos and live videos. Interactions can include actions such as likes, saves, comments, shares or replies. These metrics are estimated and in development.', 'total_value': {'value': 91}, 'id': '17841409297279102/insights/accounts_engaged/day'}, {'name': 'likes', 'period': 'day', 'title': 'Likes', 'description': 'The number of likes on your posts, reels and videos.', 'total_value': {'value': 49}, 'id': '17841409297279102/insights/likes/day'}, {'name': 'comments', 'period': 'day', 'title': 'Comments', 'description': 'The number of comments on your posts, reels, videos and live videos.', 'total_value': {'value': 100}, 'id': '17841409297279102/insights/comments/day'}, {'name': 'saves', 'period': 'day', 'title': 'Saves', 'description': 'The number of saves of your posts, reels and videos.', 'total_value': {'value': 8}, 'id': '17841409297279102/insights/saves/day'}, {'name': 'shares', 'period': 'day', 'title': 'Shares', 'description': 'The number of shares of your posts, stories, reels, videos and live videos.', 'total_value': {'value': 0}, 'id': '17841409297279102/insights/shares/day'}, {'name': 'replies', 'period': 'day', 'title': 'Replies', 'description': 'The number of replies that you received from your story, including text replies and quick reaction replies.', 'total_value': {'value': 0}, 'id': '17841409297279102/insights/replies/day'}, {'name': 'follows_and_unfollows', 'period': 'day', 'title': 'Follows and unfollows', 'description': 'The number of accounts that followed you and the number of accounts that unfollowed you or left Instagram in the selected time period.', 'id': '17841409297279102/insights/follows_and_unfollows/day'}, {'name': 'profile_links_taps', 'period': 'day', 'title': 'Profile links taps', 'description': 'The number of taps on your business address, call button, email button and text button.', 'total_value': {'value': 1}, 'id': '17841409297279102/insights/profile_links_taps/day'}], 'paging': {'previous': 'https://graph.facebook.com/v21.0/17841409297279102/insights?access_token=EABCSxik8s3MBOZBM9ycpH3GLqJci5a55fL4DUDO9ORQBZCZA0VlS9M7fNemPKVwu3XBLKkISMSZBUdPnIYQKnHZBxGjY0IKmZAvdTjhhNiJnopyv74kpJlG3Bb5c5TO3KRiBUPs6Pl4ai1qpyVvgx7eBYkDG5xc1KCpApM12KtfqUMdPObXAN2LdYJ9qz8r1gUlZATRy5ZBEs5Vqnj7L1HCROjucFTPTb6lswRus1uZBF4wZDZD&metric=impressions%2Creach%2Ctotal_interactions%2Caccounts_engaged%2Clikes%2Ccomments%2Csaves%2Cshares%2Creplies%2Cfollows_and_unfollows%2Cprofile_links_taps&metric_type=total_value&period=day&since=1740926030&until=1741098830', 'next': 'https://graph.facebook.com/v21.0/17841409297279102/insights?access_token=EABCSxik8s3MBOZBM9ycpH3GLqJci5a55fL4DUDO9ORQBZCZA0VlS9M7fNemPKVwu3XBLKkISMSZBUdPnIYQKnHZBxGjY0IKmZAvdTjhhNiJnopyv74kpJlG3Bb5c5TO3KRiBUPs6Pl4ai1qpyVvgx7eBYkDG5xc1KCpApM12KtfqUMdPObXAN2LdYJ9qz8r1gUlZATRy5ZBEs5Vqnj7L1HCROjucFTPTb6lswRus1uZBF4wZDZD&metric=impressions%2Creach%2Ctotal_interactions%2Caccounts_engaged%2Clikes%2Ccomments%2Csaves%2Cshares%2Creplies%2Cfollows_and_unfollows%2Cprofile_links_taps&metric_type=total_value&period=day&since=1741271632&until=1741444432'}}\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'values'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Nombre del archivo CSV\u001b[39;00m\n\u001b[0;32m    106\u001b[0m CSV_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m34695\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - La Salle\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlordconejo_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmerged_lordconejo_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[8], line 82\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m all_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([all_data, new_data], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Obtener la URL de la siguiente p√°gina si existe\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_data\u001b[39m(data):\n\u001b[1;32m---> 27\u001b[0m     insights \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     28\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(insights)\n\u001b[0;32m     29\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "\u001b[1;31mKeyError\u001b[0m: 'values'"
          ]
        }
      ],
      "source": [
        "import requests \n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy \n",
        "\n",
        "# Funci√≥n para obtener datos desde la API\n",
        "def fetch_insights(url, params):\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            print(response.json())\n",
        "            return response.json()\n",
        "        elif response.status_code == 400:\n",
        "            error_data = response.json().get(\"error\", {})\n",
        "            if error_data.get(\"code\") == 100 and \"last 30 days excluding the current day\" in error_data.get(\"message\", \"\"):\n",
        "                print(\"La m√©trica 'follower_count' solo est√° disponible para los √∫ltimos 30 d√≠as. Revisa el rango de fechas.\")\n",
        "            else:\n",
        "                raise Exception(f\"Error desconocido: {response.status_code}, {response.text}\")\n",
        "        else:\n",
        "            raise Exception(f\"Error al consultar la API: {response.status_code}, {response.text}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise Exception(f\"Error en la conexi√≥n: {e}\")\n",
        "\n",
        "# Funci√≥n para procesar los datos de la API y convertirlos al formato solicitado\n",
        "def process_data(data):\n",
        "    insights = data[\"data\"][0][\"values\"]\n",
        "    print(insights)\n",
        "    df = pd.DataFrame(insights)\n",
        "    df[\"end_time\"] = pd.to_datetime(df[\"end_time\"])\n",
        "\n",
        "    # Transformar al formato solicitado\n",
        "    df[\"Date\"] = df[\"end_time\"].dt.strftime('%d/%m/%Y')\n",
        "    df[\"Month\"] = df[\"end_time\"].dt.strftime('%B')\n",
        "    df[\"Day of the Week\"] = df[\"end_time\"].dt.strftime('%A')\n",
        "    df.rename(columns={\"value\": \"Daily Followers\"}, inplace=True)\n",
        "    df[\"Accumulated Followers\"] = None  # Inicializar vac√≠o\n",
        "\n",
        "    # Reordenar columnas\n",
        "    df = df[[\"Month\", \"Date\", \"Day of the Week\", \"Daily Followers\", \"Accumulated Followers\"]]\n",
        "    return df\n",
        "\n",
        "# Funci√≥n para actualizar o crear el CSV\n",
        "def update_csv(new_data, csv_file):\n",
        "    os.makedirs(os.path.dirname(csv_file), exist_ok=True)\n",
        "\n",
        "    # Cargar el archivo existente si est√° disponible\n",
        "    if os.path.exists(csv_file):\n",
        "        existing_data = pd.read_csv(csv_file)\n",
        "        existing_data[\"Date\"] = pd.to_datetime(existing_data[\"Date\"], format='%Y-%m-%d')\n",
        "    else:\n",
        "        existing_data = pd.DataFrame(columns=[\"Month\", \"Date\", \"Day of the Week\", \"Daily Followers\", \"Accumulated Followers\"])\n",
        "\n",
        "    # Convertir las fechas en el nuevo conjunto de datos al mismo formato\n",
        "    new_data[\"Date\"] = pd.to_datetime(new_data[\"Date\"], format='%d/%m/%Y')\n",
        "\n",
        "    # Reemplazar valores de `Daily Followers` con 0 por nuevos datos si son diferentes de 0\n",
        "    for index, row in new_data.iterrows():\n",
        "        matching_row = existing_data[existing_data[\"Date\"] == row[\"Date\"]]\n",
        "        if not matching_row.empty:\n",
        "            if matching_row.iloc[0][\"Daily Followers\"] == 0 and row[\"Daily Followers\"] != 0:\n",
        "                existing_data.loc[matching_row.index, \"Daily Followers\"] = row[\"Daily Followers\"]\n",
        "        else:\n",
        "            # Si no hay fila existente para esta fecha, a√±adirla\n",
        "            existing_data = pd.concat([existing_data, pd.DataFrame([row])], ignore_index=True)\n",
        "\n",
        "      # Ordenar por fecha y guardar el resultado en el archivo CSV\n",
        "    combined_data = existing_data.sort_values(\"Date\").drop_duplicates(subset=\"Date\")\n",
        "    combined_data[\"Accumulated Followers\"] = combined_data[\"Daily Followers\"].cumsum() + 5764\n",
        "    combined_data.to_csv(csv_file, index=False)\n",
        "    return combined_data\n",
        "\n",
        "\n",
        "# Funci√≥n principal para manejar la paginaci√≥n y guardar datos\n",
        "def main():\n",
        "    url = BASE_URL\n",
        "    all_data = pd.DataFrame(columns=[\"Date\",\"Day\",\"Impressions\",\"Reach\",\"Total_interactions\",\"Accounts_engaged\",\"Likes\",\"Comments\",\"Saves\",\"Shares\",\"Replies\",\"Follows_and_unfollows\",\"Profile_links_taps\"])\n",
        "\n",
        "    while url:\n",
        "        data = fetch_insights(url, PARAMS)\n",
        "        if not data:\n",
        "            break\n",
        "        new_data = process_data(data)\n",
        "        all_data = pd.concat([all_data, new_data], ignore_index=True)\n",
        "\n",
        "        # Obtener la URL de la siguiente p√°gina si existe\n",
        "        url = data.get(\"paging\", {}).get(\"previous\", None)\n",
        "\n",
        "    # Actualizar o crear el archivo CSV\n",
        "    updated_data = update_csv(all_data, CSV_FILE)\n",
        "    print(\"Datos actualizados exitosamente.\")\n",
        "    print(updated_data)\n",
        "\n",
        "# Ejecutar el script\n",
        "if __name__ == \"__main__\":\n",
        "    # URL base de la API\n",
        "    BASE_URL = \"https://graph.facebook.com/v21.0/17841409297279102/insights\"\n",
        "    ACCESS_TOKEN = \"EABCSxik8s3MBOZBM9ycpH3GLqJci5a55fL4DUDO9ORQBZCZA0VlS9M7fNemPKVwu3XBLKkISMSZBUdPnIYQKnHZBxGjY0IKmZAvdTjhhNiJnopyv74kpJlG3Bb5c5TO3KRiBUPs6Pl4ai1qpyVvgx7eBYkDG5xc1KCpApM12KtfqUMdPObXAN2LdYJ9qz8r1gUlZATRy5ZBEs5Vqnj7L1HCROjucFTPTb6lswRus1uZBF4wZDZD\"\n",
        "    PARAMS =  {\n",
        "        \"metric\": \"impressions,reach,total_interactions,accounts_engaged,likes,comments,saves,shares,replies,follows_and_unfollows,profile_links_taps\",\n",
        "        \"period\": \"day\",\n",
        "        \"metric_type\": \"total_value\",\n",
        "        \"access_token\": ACCESS_TOKEN,\n",
        "    }\n",
        "\n",
        "    # Nombre del archivo CSV\n",
        "    CSV_FILE = r\"C:\\Users\\34695\\OneDrive - La Salle\\Documents\\lordconejo_data\\Data\\merged_lordconejo_data.csv\"\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
